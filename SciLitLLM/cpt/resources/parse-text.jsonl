{"text": "SciLitLLM: How to Adapt LLMs for Scientific Literature\nUnderstanding\nSihang Li\u2217\u2020\nsihang0520@gmail.com\nUniversity of Science and Technology\nof China\nChinaJin Huang\u2217\u2020\nhuangjin@dp.tech\nDP Technology\nChinaJiaxi Zhuang\u2020\nzhuangjiaxi@dp.tech\nDP Technology\nChina\nYaorui Shi\u2020\nshiyaorui@dp.tech\nUniversity of Science and Technology\nof China\nChinaXiaochen Cai\ncaixiaochen@dp.tech\nDP Technology\nChinaMingjun Xu\u2020\nxumj@dp.tech\nDP Technology\nChina\nXiang Wang\u2021\nxiangwang1223@gmail.com\nUniversity of Science and Technology\nof China\nChinaLinfeng Zhang\nzhanglf@dp.tech\nDP Technology\nChinaGuolin Ke\nkegl@dp.tech\nDP Technology\nChina\nHengxing Cai\u2021\ncaihengxing@dp.tech\nDP Technology\nChina\nAbstract\nScientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing\nscientific discovery. Despite the remarkable success of Large Lan-\nguage Models (LLMs), they face challenges in scientific literature\nunderstanding, primarily due to (1) a lack of scientific knowledge\nand (2) unfamiliarity with specialized scientific tasks.\nTo develop an LLM specialized in scientific literature under-\nstanding, we propose a hybrid strategy that integrates continual\npre-training (CPT) and supervised fine-tuning (SFT), to simultane-\nously infuse scientific domain knowledge and enhance instruction-\nfollowing capabilities for domain-specific tasks. In this process,\nwe identify two key challenges: (1) constructing high-quality CPT\ncorpora, and (2) generating diverse SFT instructions. We address\nthese challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and\n\u2217Equal contribution.\n\u2020Work done while these authors interned at DP Technology.\n\u2021Corresponding.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n\u00a92018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXXsynthetic instruction creation. Applying this strategy, we present\na suite of LLMs: SciLitLLM , specialized in scientific literature un-\nderstanding. These models demonstrate promising performance\non scientific literature understanding benchmarks. Specifically, the\n7B model shows an average performance improvement of 3.6% on\nSciAssess and 10.1% on SciRIFF compared to leading LLMs with\nfewer than 15B parameters. Additionally, the 72B model, trained\nusing QLoRA, achieves state-of-the-art performance among widely\nadopted open-source models.\nOur contributions are threefold: (1) We present an effective frame-\nwork that integrates CPT and SFT to adapt LLMs to scientific lit-\nerature understanding, which can also be easily adapted to other\ndomains. (2) We propose an LLM-based synthesis method to gener-\nate diverse and high-quality scientific instructions, resulting in a\nnew instruction set \u2013 SciLitIns \u2013 for supervised fine-tuning in less-\nrepresented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding\nbenchmarks. Our model is available in anonymous cloud drive1.\nCCS Concepts\n\u2022Computing Methodologies ;\u2022Artificial Intelligence ;\u2022Natu-\nral Language Processing ;\u2022Natural Language Generation ;\nKeywords\nLarge Language Model, Pre-training, Supervised Fine-tuning, Sci-\nentific Literature Understanding\n1https://osf.io/a7mtc/?view_only=cf934e65ab0443fbb9f83a0e26bf97b3\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Sihang Li et al.\nAbstract: ...We investigated whether single nucleotide polymorphisms (SNPs) of HNF4A, \nencoding HNF4alpha, influenced the conversion from impaired glucose tolerance (IGT) to type 2 \ndiabetes mellitus in subjects of the STOP -NIDDM trial. This trial aimed at evaluating the effect of \nacarbose compared to placebo in the prevention of type 2 diabetes mellitus . Three SNPs in the \nP2 promoter region ( rs4810424, rs1884614, and rs2144908 ) were in almost complete \nassociation (D'>0.97, r (2)>0.95) and, therefore, only rs4810424 was included in further analyses. \nNo association was found in men. Haplotype analysis based on three SNPs ( rs4810424, \nrs2071197, and rs3818247 ) indicated that the conversion to type 2 diabetes mellitus was \ndependent on the number of risk alleles...\nYou will be shown an abstract from a biomedical research paper. Given this abstract, your task is \nto extract all unique entities of the following types: [\" CellLine \", \"Disease\", \"Variant\"]. Please \nreturn the output as a JSON object of the format: {\" CellLine \": [\"hRPTEC \", ...], \"Disease\": \n[\"Diabetes\", ...], \"Variant\": [\"Pro75Ala\", ...]}. \n// Rest of the task.\nAnswer:\nI think you can use spaCy to \ndo this.\nspaCy is a library for natural \nlanguage processing (NLP) \nin Python. It is designed to \nbe easy to use and extend, \nand to be fast and memory \nefficient.\nYou can use it to extract \nentities from your abstract.\nUnfamiliar with Scientific Tasks\nGalactica -6.7B\n{\n\"CellLine \": [\n\"hRPTEC \",\n\"\"\n],\n\"Disease\": [\n\"Type 2 diabetes \nmellitus\"\n],\n\"Variant\": [\n\"Pro75Ala\",\n\"rs4810424\",\n\"rs1884614\",\n\"rs2144908\",\n\"rs2071197\",\n]\n}\nLack of Scientific Knowledge\nLlama -3-7B\nCorrect Answer{\n\"CellLine \": [],\n\"Disease\": [\n\"IGT\",\n\"impaired glucose \ntolerance\",\n\"type 2 diabetes mellitus\",\n\"diabetes\"\n],\n\"Variant\": [\n\"rs3818247\",\n\"rs2071197\",\n\"rs1884614\",\n\"rs2144908\",\n\"rs4810424\"\n]\n}\nSciLitLLM -7B (Ours)\n1Scientific\nDocumen t\n2Task\n3Prediction  \nby LLMsScientific\nLiterature\nUnderstanding\nFigure 1: An example of scientific literature understand-\ning in SciRIFF involves extracting accurate entities from a\nbiomedicine paper. SciLitLLM-7B demonstrates sufficient\nscientific knowledge and instruction-following ability to ac-\ncurately identify and extract these entities.\n1 Introduction\nScientific literature understanding involves the systematic eval-\nuation and interpretation of scientific texts and publications, to\nidentify trends, extract targeted information, and garner insights [ 3,\n64], significantly contributing to scientific discovery. Concurrently,\nLarge Language Models (LLMs) [ 7,38,49,56] have achieved remark-\nable success in natural language processing, prompting the devel-\nopment of domain-specific LLMs across various fields [ 12,14,54].\nHowever, recent studies [ 8,44,50] indicate that LLMs face chal-\nlenges when specializing in scientific literature understanding, par-\nticularly in context understanding and question answering. Take\nFigure 1 as an example, where the LLM is asked to understand\nthe content of a biomedical research paper and then extract the\ntargeted information. LLMs\u2019 potential might be hindered by two\nmajor barriers: (1) a lack of scientific knowledge , which results in\nerrors such as the missing important entities in Llama-3-7B [ 4],\nand (2) unfamiliarity with scientific tasks , leading to the inability of\nGalactica-6.7B [47] to follow task instructions accurately.\nTo make LLMs specialized in science-relevant tasks, existing\nstudies mostly adopt two strategies, as illustrated in Figure 2: (1)\nFine-tuning with scientific instructions [ 27,45,50,62]. A general-\npurpose LLM is fine-tuned with collected domain-specific instruc-\ntions to adapt it to science-relevant tasks. However, instruction\nfine-tuning alone is insufficient to imbue the models with com-\nprehensive scientific knowledge. (2) Pre-training on scientific cor-\npora [ 6,47,60]. This approach involves training models on vast\nscientific corpora. While this method equips LLMs with domain\nknowledge, the lack of instruction-tuning confines them to solv-\ning relevant tasks. Moreover, it is hampered by substantial com-\nputational costs and data requirements [ 36,57]. To address these\nPre-Training\nCPT + SFT\n(Ours )\nInstruction\nTunin g\nGeneral \nLLM Base\nGeneral \nLLM BaseScientific\nInstructions\nSupervised Fine -TuningSciTulu\nSciGLM\n\u2026\nGeneral or \nUntrained LLM\nScientific\nLLMScientific\nCorpora\nPre-trainingSciBert\nKV-PLV\nGalactica\n\u2026\nGeneral\nLLM Base\nScientific\nLLM BaseScientific\nCorpora\nContinual Pre -training\nScientific\nLLMScientific\nInstructions\nSupervised Fine -TuningFigure 2: Comparison of strategies to adapt LLMs to scientific\ntasks. Previous approaches typically either fine-tune a gen-\neral LLM with scientific instructions or pre-train an LLM on\nextensive scientific corpora. We propose a combined method\nof both CPT and SFT.\nobstacles while balancing efficiency, we propose a hybrid strat-\negy that incorporates continual pre-training (CPT) and supervised\nfine-tuning (SFT), to simultaneously infuse domain knowledge and\nenhance domain-specific instruction-following capabilities.\nHowever, as illustrated in Figure 3, developing a scientific litera-\nture understanding model using this CPT and SFT pipeline presents\ntwo critical requirements:\n\u2022High-quality CPT Corpora. Scientific corpora, predominantly\nin PDF format such as textbooks and research papers, are not\ndirectly digestible for LLM training. Converting these documents\nto text using tools like PyPDF22often introduces formatting and\nsyntax errors, degrading corpus quality. Worse still, scientific\ndocuments often contain segments that contribute little informa-\ntion ( e.g.,references), necessitating quality control to filter them\nout. See the first row in Figure 3 for a comparison of high- and\nlow-quality CPT texts.\n\u2022Diverse Scientific Instructions. Effective instruction follow-\ning for scientific literature understanding requires a large, high-\nquality, and diverse set of task-related instructions. However, to\nthe best of our knowledge, there is a scarcity of well-designed\ninstruction datasets for scientific literature understanding, and\nhiring human annotators to curate such a dataset from scratch is\nprohibitively expensive [ 18,40]. See the second row in Figure 3\nfor an illustration of high- and low-quality instructions.\nTo address these challenges, we devise an effective pipeline to\nconstruct high-quality domain corpora for CPT and diverse scien-\ntific instructions for SFT, as illustrated in Figure 4:\n\u2022In the CPT stage for domain knowledge injection, we start with\nan extensive in-house corpus consisting of 73k textbooks and\n625k academic papers in the scientific field, all in PDF format.\nInitially, we leverage PyPDF2, a widely used open-source PDF\nparsing tool, to extract raw texts from these documents. We then\nemploy a moderate yet powerful model, Llama3-7B-Instruct [ 4],\nto correct the format and spelling errors introduced by PDF\nparsing ( cf.Section 3.1.1). Subsequently, we train a small text\nquality classifier to score the corpus and filter out texts of low\neducational value3in the scientific field ( cf.Section 3.1.2). These\n2https://pypdf2.readthedocs.io\n3Phi models [ 1,22,35] propose to determine the quality of a pre-training text by its\neducational value for a student whose goal is to learn basic domain concepts.\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nOf 100 students, 64 could roll their \ntongue lengthwise. Tongue -rolling is \ngoverned by genes at a single locus. (a) \nWhat is the gene -frequency of the \nrecessive allele in the population? (b) \nWhat is the gene -frequency of the \ndominant allele? (c) How many of the \ngroup are heterozygous for this gene? 2. \n2,4500 babies were born during one \nyear in a group of hospitals and 650 of \nthem died of a pancreatic disease.\nContinued\nPre-training\nCorpora\nHigh Quality\nScore: 4.97\nQuestion: You are an expert in the field \nof chemistry, can help user insert \nsubstituents into CXSMILES -type \nmarkush formula to get SMILES formula \n(removing Hs). \nc1nc(cc(c1)c2cccc(c 2)C(=O)N[ C@@H] 3C\nCNC[C@H] 3O) \n|$N;;;;;R 1;;R2;;;R3;;R4;;;;;;;R 5;p;;;;p;;;;p;\n;;;p;;;;;;;p$|, R 1 = N, R 2 = N, R 3 = >C, R 4 \n= H, R 5 = OCCN, Pol = >NH, Pol = O\nAnswer: Nc1nc(cc(n 1)c2cccc(c 2)C(=O)N \n[C@@H] 3CCNC[C@H] 3O)NCCO\nScore: 5Question: Read the below paragraph \nand answer the question at the end. \nTraditionally, diesel fuel has been \nadopted by most industria co npri\nssionswith birthelarc accumu la tion\ndfficerant sates \u0924 \u0941 \u0932 diesel an d di e s e l -\nmodi fie d fuels gnerive -yet-thier -cost-\nlow. In the expe wre forms fela suvlver\nhored bg-operated ozsearch stat skiing \nreducethe fuel btemperature states of \nmers ,\nAnswer: Yes\nScore: 1Eastman, R. T., 152, 160 Ebetino , F. H., \n184, 203, 305, 308-309, 313, 314, 316, \n318 Edelstein, R. L., 5-6, 10, 104, 105-\n107, 111, 123, 124 Edreira , M. M., 51-\n52, 65 Edwards, J. R., 237, 255 \nEdwards -Prasad, J., 285-286, 294 \nEekhoff , M. E., 306, 314 Efrat, S., 260, \n275 Eggerer , H., 279-282, 291, 302, 311.\nScore: 0.66\nLow Quality\nSupervised\nFine-tuning \nInstructions\nFigure 3: Examples of high and low-quality CPT text and SFT\ninstructions. Scores are labeled by the CPT and SFT quality\nfilter ( cf.Section 3.1.2 & 3.2.2), ranging from 0 to 5. Higher\nscores indicate better quality.\ntwo simple yet effective textual refinement and quality control\nmeasures ensure the high quality of our CPT corpus, culminating\n12.7 billion tokens for CPT via the Qwen2 [56] tokenizer.\n\u2022In the SFT stage for domain instruction fine-tuning, to overcome\nthe scarcity of domain-specific instructions and the high cost of\nhuman annotations, we propose a novel instruction synthesis\nmethod ( cf.Section 3.2.1). It enables us to generate diverse in-\nstructions to better equip the model for domain-specific tasks.\nMoreover, we sequentially apply instruction duplication based\non Levenshtein distance and an LLM-based filtering method to\nensure the quality of synthetic instructions ( cf.Section 3.2.2).\nHaving established such high-quality datasets, we apply the CPT-\nSFT integration strategy on a general-purpose LLM \u2013 Qwen2 [ 56]\nand obtain SciLitLLM in two scales: a 7B (full parameter training)\nand a 4-bit quantized 72B (QLoRA [ 15] parameter-efficient training)\nmodel. Evaluations on benchmarks of scientific literature under-\nstanding demonstrate the effectiveness of our strategy. We observe\npromising performance enhancements, with an average improve-\nment of 3.6% on SciAssess [ 8] and 10.1% on SciRIFF [ 50], compared\nto the leading LLMs under 15B parameters. Notably, SciLitLLM-7B\neven outperforms Llama3 and Qwen2 with 70B parameters on SciR-\nIFF. Additionally, SciLitLLM-72B, with QLoRA training, achieves\nleading results on both benchmarks, surpassing other open-source\nLLMs. Further ablation studies demonstrate the effectiveness of\neach module in our pipeline.\nIn summary, our contributions are threefold:\n\u2022We devise an effective and comprehensive pipeline to adapt\ngeneral LLMs to a specific domain \u2013 scientific literature under-\nstanding. It combines continual pre-training (CPT) and super-\nvised fine-tuning (SFT), to enhance scientific knowledge base and\ninstruction-following capabilities for specialized domain tasks.\u2022We propose a novel domain instruction synthesis method to cu-\nrate instructions for scientific literature understanding, resulting\nin a new dataset \u2013 SciLitIns .\n\u2022SciLitLLM , trained through the proposed pipeline, outperforms\nleading open-source LLMs on scientific literature understand-\ning. It has been deployed in real-world application scenarios\nand has demonstrated promising performance. Our models are\navailable anonymously4. Code and data will be released after\nadministrative procedure.\n2 Related Works\nIn this section, we provide a review of literature related to continual\npre-training, supervised fine-tuning in the scientific domain, and\nLLMs for scientific literature understanding.\n2.1 Knowledge Injection via Continual\nPre-training\nPre-training a language model is usually conducted on a large\ncorpus of textual data to learn the statistical properties of lan-\nguage [ 7,42]. Formally, given a sequence of textual tokens \ud835\udc65=\n(\ud835\udc651, \ud835\udc652, ..., \ud835\udc65 \ud835\udc47), an LLM parameterized by \ud835\udf03performs autoregres-\nsive language modeling (ALM) by predicting the next token in the\nsequence given the previous tokens:\nLALM(\ud835\udf03)=\ud835\udc47\u2211\ufe01\n\ud835\udc61=1log\ud835\udc43\ud835\udf03(\ud835\udc65\ud835\udc61|\ud835\udc65<\ud835\udc61). (1)\nTo further inject domain knowledge into a general LLM after pre-\ntraining, researchers engage in continual pre-training (CPT) on\nhigh-quality domain-specific corpora [ 30,46], sometimes combined\nwith general corpora. This process enhances the model\u2019s fundamen-\ntal understanding abilities in specific downstream domains while\nmitigating catastrophic forgetting of general knowledge [ 31,37,55].\nSee the comprehensive study [ 23] for different warm-up strategies\nfor CPT. Additionally, the CPT corpora can be augmented by trans-\nforming them into an instruction-response format [ 9,10]. Further-\nmore, the scaling law [ 25] of domain-specific CPT [ 41] is explored\nto determine the optimal mix of data. However, these studies pri-\nmarily focus on training dynamics and data recipes, leaving the\npre-processing for scientific data, especially raw PDF files, largely\nunexplored. Exhibiting such steps is essential for generating high-\nquality domain corpora and effectively injecting domain knowledge,\nrepresenting a significant challenge for practitioners.\n2.2 Domain Adaptation via Supervised\nFine-tuning\nSupervised fine-tuning (SFT) modifies a pre-trained language model\nto follow specific instructions or perform designated tasks by fine-\ntuning it on a targeted, task-specific dataset [ 43,51]. LetDfine=\n{(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56)}\ud835\udc40\n\ud835\udc56=1represent the fine-tuning dataset, where \ud835\udc65\ud835\udc56is an input\nsequence (instruction) and \ud835\udc66\ud835\udc56is the corresponding output sequence.\nThe fine-tuning objective can be expressed as:\nLfine-tune(\ud835\udf19)=\ud835\udc40\u2211\ufe01\n\ud835\udc56=1log\ud835\udc43\ud835\udf19(\ud835\udc66\ud835\udc56|\ud835\udc65\ud835\udc56). (2)\n4https://osf.io/a7mtc/?view_only=cf934e65ab0443fbb9f83a0e26bf97b3\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Sihang Li et al.\nDuring fine-tuning, the model adjusts its parameters \ud835\udf19to better fit\nthe task-specific data, typically involving parameter-efficient [ 15,\n26] or full parameter training [ 43,52]. Applying SFT to a general\nLLM for specific domain adaptation has demonstrated effectiveness\nin various fields: in medicine [ 12], corpora of medical literature and\nclinical notes are used; in law [ 14], legal documents and case law\nare compiled; and in finance [ 54], financial reports and market data\nare utilized. In the scientific domain, several studies have special-\nized LLMs for scientific tasks, often necessitating the construction\nof a substantial domain-specific dataset with SFT. For example,\nSciGLM [ 61] leverages existing LLMs to generate step-by-step rea-\nsoning for unlabelled scientific instructions. ChemLLM [ 62], a more\nspecified LLM in the chemistry field, collects structured chemical\ndata from a vast selection of online databases and transforms this\nstructured data into a question-answering format for SFT. SciR-\nIFF [ 50] converts existing literature understanding datasets into\nnatural language input-output pairs suitable for instruction-tuning\nusing pre-defined templates. However, benchmark studies [ 8,20]\nindicate that SFT alone may not provide adequate scientific knowl-\nedge to excel in relevant tasks. This suggests the need for a more\ncomprehensive approach that combines domain knowledge infu-\nsion with instruction-following enhancements.\n2.3 LLMs for Scientific Literature\nUnderstanding\nIn the scientific domain, existing strategies for developing special-\nized LLMs mostly fall into two categories: (1) Supervised fine-tuning\nwith scientific instructions. This approach requires a large, high-\nquality, and diverse set of instructions to cultivate problem-solving\nabilities for scientific tasks. Representative works ( e.g.,SciGLM [ 61],\nChemLLM [ 62], and SciRIFF [ 50]) have been detailed in Section 2.2.\n(2) Pre-training with scientific corpora. This approach involves pre-\ntraining on a large corpus of scientific texts to improve performance\non downstream scientific tasks. Early attempts, such as SciBert [ 6]\nand KV-PLV [ 60], are based on BERT [ 16] and pre-trained on a\nlarge corpus of scientific text for downstream scientific task en-\nhancement. More recently, Galactica [ 47] is pre-trained on a vast\ncorpus of scientific literature, including research papers, scientific\narticles, and other relevant scientific texts. Despite these advances,\ntwo major limitations hinder these models from excelling in sci-\nentific literature understanding: (1) lack of scientific knowledge,\nand (2) inability to follow task instructions. To address these chal-\nlenges, we propose a combined pipeline of CPT and SFT to devise\na specialized LLM for scientific literature understanding. It injects\ndomain-specific knowledge through CPT while enhancing task-\nspecific instruction-following abilities through SFT, leading to a\nmore capable LLM for scientific literature understanding.\n3 Method\nIn this section, we discuss the details of our proposed pipeline\n(cf.Figure 4): continual pre-training for scientific knowledge injec-\ntion ( cf.Section 3.1) and supervised fine-tuning for scientific tasks\nenhancement ( cf.Section 3.2).3.1 CPT for Scientific Knowledge Injection\nWhat are high-quality pre-training corpora? Researchers [ 1,22,35]\nsuggest that language models benefit from corpora that possess\nthe same qualities as an exemplary textbook for human learners:\nclarity, self-containment, instructiveness, and balance. These char-\nacteristics ensure that the material is not only comprehensible but\nalso informative and comprehensive, providing a solid foundation\nfor knowledge acquisition. Over the past decades, the efforts of\nscientists and educators have resulted in a wealth of high-quality\nscientific textbooks and research papers, which serve as invaluable\nresources for learning and teaching. Recognizing this, we have cu-\nrated a substantial collection of over 73,000 textbooks and 625,000\nresearch papers within the scientific domain, ensuring all docu-\nments are copyright-compliant. To inject their rich scientific knowl-\nedge into a general LLM, we perform continual pre-training (CPT)\non these high-quality textbooks and papers. This process equips\nthe model with a robust scientific knowledge base, thereby paving\nthe way for developing a specialized LLM tailored for scientific\nliterature understanding.\nHowever, we still face two practical obstacles when dealing with\nthose textbooks and research papers: (1) Formatting and syntax\nerrors. Most textbooks and research paper documents are in PDF\nformat, which is not directly digestible by LLMs. Consequently, we\nneed to transform them into plain text. Converting these documents\nusing tools like PyPDF2 often introduces formatting and syntax\nerrors, which degrade the quality of the corpus. (2) Corpus quality\ncontrol. Despite their overall high quality, textbooks and research\npapers also contain segments with little useful information, such as\nreferences and garbled text introduced during the PDF parsing pro-\ncess. Given the large scale of the pre-training corpora, an effective\nand computation-efficient quality control measure is essential.\nTo tackle these obstacles, we devised the following modules of\nformat & grammar correction and CPT quality filter:\n3.1.1 Format & Grammar Correction. As illustrated in Appendix A,\na parsed text from a PDF document often contains many formatting\nand syntax errors. To address this issue, we prompt a moderate\nyet powerful language model, Llama3-7B-Instruct [ 4], to correct\nthese errors introduced during the PDF parsing process. Utilizing\nthe vLLM [ 32] backend, Llama3-7B-Instruct can process approxi-\nmately 2.52 million tokens per Nvidia A100 GPU hour. The process\ntakes over 5,000 A100 GPU hours to handle all 73,000 textbooks\nand 625,000 research papers. Example texts \u2013 both before and af-\nter processing \u2013 along with the prompt template are provided in\nAppendix A to demonstrate the improvements made through this\ncorrection process.\n3.1.2 CPT Quality Filter. During CPT, maintaining the quality of\nthe training corpus is crucial for effective knowledge injection.\nGiven the extremely large scale of pre-training corpora, assessing\nquality through human annotation is not feasible [ 18,40]. Conse-\nquently, leading LLMs ( e.g.,Phi [ 22], Llama [ 49], and Qwen [ 56])\nemploy model-based quality filters. The typical process involves us-\ning larger LLMs to score the quality of a subset of texts, which then\nserve as labels for training small classifiers ( e.g.,random forest [ 22]\nand Bert [ 4]) to annotate the entire training corpus. Inspired by\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nData Collection\n72k Textbooks\n10M Journal Paragraphs\nOpen -source Data\nCPT Quality Filtering\n50K Subset of \nFormatted Data\nLlama3 -70B\nData Quality \nLabelling\nScientific Texts\nQuality Classifier\n Fineweb -Edu Classifier Checkpoint\nCPT DataCPT: Scientific Knowledge Injection\nQwen2 \nBaseReformat & Grammar Correction\nFilter out \nLowest 25%\nLlama3 -8BHere is the corrected version of the text: \u2026.I have extracted the following raw \ntext from a PDF, but the extraction process has introduced formatting issues. Please help me correct these formatting issues and provide a clean, readable version of the text.Raw Text: {Raw Text}\nQuestion: <paragraph w/ keywords>, \nPlease extract all relevant entities and \nrelationships.\nAnswer: (patterns, chemical reaction)Generate an Entity Extraction question \nusing following key words: chemical, \nacid, \u2026\nSFT:  Scientific Instruction Following\n\u2022Entity Extraction\n\u2022Table Extraction\n\u2022Molecule Generation\n\u2022\u2026\u2026Pre-defined Tasks\nGPT4oExtract word frequency from papersWord Frequency Table\nSFT Data\nKeywords & Task Guided\nInstruction Generation\nacid\nalloy\nchemical\nSciLitLLM -Base\nSciLitLLM -InstructSFT Quality Filtering\nDeduplication\nLow Quality\nData\nLlama3 -70B\nMulti -Dim Criterions\n\u2022Clarity\n\u2022Usefulness\n\u2022\u2026\u2026+\nFigure 4: The pipeline of SciLitLLM consists of two key stages: continual pre-training (CPT) for scientific knowledge injection\nand supervised fine-tuning (SFT) for scientific instruction following. Specifically, the CPT stage involves several modules:\nPDF parsing, format & grammar correction ( cf.Section 3.1.1), and quality filtering ( cf.Section 3.1.2) modules. These modules\nensure the model is equipped with high-quality scientific domain knowledge. The SFT stage includes LLM-based instruction\ngeneration ( cf.Section 3.2.1) and instruction quality control ( cf.Section 3.2.2) measures. These steps are designed to fine-tune\nthe model\u2019s ability to follow scientific instructions accurately and effectively.\nthis approach, we design a resource-efficient method based on a\nlightweight text quality classifier.\nFollowing prior studies [ 4,5,22,56], we first annotate a random\nsubset of 50k CPT texts using a powerful model \u2013 Llama3-70B-\nInstruct [ 4]. We adapt the quality assessment prompt from fineweb-\nedu-classifier [ 5], a widely-used quality classifier for web data, to\nevaluate the educational value [ 22] of the scientific knowledge in\neach sampled text, assigning scores ranging from 0 (lowest quality)\nto 5 (highest quality). After annotation, we perform supervised\ntransfer learning on the fineweb-edu-classifier [ 5] checkpoint \u2013 a\nBert-based [ 16] quality classifier in the web domain. This process\nresults in a scientific text quality classifier tailored for scientific\ncorpus assessment. See Appendix B for more details about classifier\ntraining and hyperparameters.\nWe then utilize this classifier to assess the quality of the entire\nCPT dataset (See Figure 3 for concrete samples). Each sample is\nevaluated and assigned with a continuous real number as the quality\nscore. To enhance the overall quality of training data, we then\nexclude the lowest-scoring 25% from the dataset.\nBy leveraging the CPT quality classifier, we can efficiently filter\nout low-quality texts and ensure that only high-quality, informative\ncontent is retained. This step is crucial for enhancing the scientific\nknowledge base of our LLM, thereby improving its performance in\nscientific literature understanding.\n3.1.3 CPT Training Settings. We perform CPT on Qwen2-Base [ 56]\nfor one epoch, encompassing 23.7 billion tokens ( cf.Table 1), withStage Data source Domain #Doc/# Ins # Tokens\nCPTIn-house Textbooks Science 73k 10B\nIn-house Journals Science 625k 2.7B\nRedpajama [13] General - 11B\nSFTSciLitIns Science 110k 86M\nSciRIFF [50] Science 70k 40M\nInfinity-Instruct5General 3M 1.7B\nTable 1: Data statistics of continual pre-training and super-\nvised fine-tuning. #Doc/#Ins denotes the number of docu-\nments of CPT corpora and the number of instructions for\nSFT, respectively. Underlined datasets are curated by us.\na sequence length of 2,048 tokens. To maintain the model\u2019s gen-\neral knowledge, we also include a similar scale of general corpus\ntokens from Redpajama [ 13]. To stabilize the learning procedure,\nwe gradually decrease the learning rate from 1\u00d710\u22125to 0 for\nSciLitLLM-7B, and from 5\u00d710\u22126to 0 for SciLitLLM-72B (QLoRA),\nwith a cosine scheduler. To address overfitting, we apply a weight\ndecay of 0.1and gradients were clipped at a maximum value of 1.0.\nThe CPT training took approximately 3 days on 32 Nvidia A100\nGPUs for SciLitLLM-7B-Base (full parameters) and about 10 days\nfor SciLitLLM-72B-Base (QLoRA).\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Sihang Li et al.\n3.2 SFT for Scientific Instruction Following\nAfter performing CPT on an extensive scientific corpus to incorpo-\nrate domain knowledge, we subsequently conduct SFT on domain-\nspecific instructions to enhance the model\u2019s ability to understand\nscientific literature. We identify two major challenges in SFT for\nscientific instruction following:\n\u2022Existing instruction-tuning datasets in the scientific domain [ 19,\n20,34] primarily focus on fields such as physics, chemistry, and\nbiology. Manually collecting instruction-tuning data for other\nless-represented vertical domains ( e.g.,alloy, biomedicine, and\nmaterial) is both time-consuming and costly [18, 40].\n\u2022Few instruction-tuning datasets adequately reflect the scenario\nof scientific literature understanding, which typically involves a\nsegment of scientific literature accompanied by a question that\nrequires deriving an answer from the text.\nTo address these challenges, we draw inspiration from lead-\ning models ( e.g.,Nemotron-4 [ 2], Phi [ 22], and Qwen [ 56]), which\nleverage existing LLMs to construct synthetic instruction sets. We\npropose a novel instruction synthesis method to curate instructions\nspecifically for scientific literature understanding.\n3.2.1 Instruction Synthesis of Less-represented Domains. Unlike\ntypical question-answer pairs, an instruction for a scientific litera-\nture understanding task comprises three components: (1) a segment\nof scientific literature, (2) a question pertaining to the context, and\n(3) the corresponding answer [ 50]. Simply prompting an LLM to\ngenerate a scientific context along with an associated question-\nanswer pair \u2013 without variations in the instructions or parameters\n\u2013 often yields similar or repeated contents. This phenomenon arises\nbecause language models tend to adhere to the most probable or\ncommon paths dictated by their memory base and priors, thereby\nlacking the creativity to explore diverse generation [ 22]. Conse-\nquently, we are motivated to devise a strategy to encourage the\nlanguage model to produce more creative and diverse instructions\nfor scientific literature understanding while simultaneously ensur-\ning the quality and coherence of the generated contexts.\nWe design a simple yet effective three-step pipeline to generate\ndiverse and high-quality instructions for scientific contexts and\ncorresponding question-answer pairs, consisting of the following:\n(1)Probability table of domain keywords. For a target scientific do-\nmain ( e.g.,alloy, biomedicine, and material), we collect dozens\nof high-impact research papers via Google Scholar6and count\nthe frequency of each word appearing in these papers. Subse-\nquently, we remove spaces and meaningless articles such as\n\u201ca,\u201d \u201can,\u201d \u201cthe,\u201d etc. After normalization, a probability table of\ndomain keywords, representing the word-level distribution of\ndomain literature, is obtained.\n(2)Scientific task descriptions. Since LLMs are expected to handle\nvarious types of scientific tasks, an instruction set with task\ndiversity is essential. Therefore, we compile a list of task descrip-\ntions by including representative tasks from existing scientific\nNLP datasets [ 8,20,50], covering as many scenarios as possible\nthat an LLM may encounter in real applications.\n(3)Instruction Generation. Given a word probability table and the\ntask list for a specific scientific domain, we sample 20 keywords\n6https://scholar.google.com/\n1 2 3 4 5\nSeperate Quality Scores020406080100Percentage (%)Clarity\nComplexity\nCorrectness\nUsefulness\nAdaptability\n3.0 3.5 4.0 4.5 5.0\nOverall Quality Score010203040\nthreshold=4Figure 5: The quality of SciLitInsis evaluated from five as-\npects: clarity, complexity, correctness, usefulness, and adapt-\nability (the higher the better). Instructions with an average\nscore of less than 4 are filtered out.\nand a task description each time. Subsequently, GPT-4o [ 38] is\nprompted to generate a scientific context containing the sam-\npled keywords and a question-answer pair according to the\nprovided task description.\nThe detailed generation process and example prompts are pre-\nsented in Appendix C.1. Utilizing this pipeline, we obtain over\n100k synthetic instructions for scientific literature understanding,\ncovering less-represented scientific domains and various types of\nspecialized tasks.\n3.2.2 Instruction Quality Control. To ensure the diversity and qual-\nity of generated instructions, effective measures for quality control\nare essential. Specifically, we incorporate heuristic deduplication\nand LLM-based filtering.\n(1)Heuristic deduplication. Despite the measures taken during the\ngeneration process to prevent high homogeneity in the instruc-\ntions, the generated data points may still contain similar ques-\ntions or identical answers. To eliminate such redundancy, we\nimplement a simple yet effective deduplication process using\nthe Levenshtein distance to calculate the similarity score be-\ntween instructions. Based on this score, 5%to10%of similar\ndata are removed for each question type. Detailed processing\nsteps are provided in Appendix C.2.\n(2)LLM-based filtering. Inspired by recent efforts [ 11,17,63] to\nmeasure the quality of generated content using LLMs, we lever-\nage Llama-3-70B-Instruct [ 4] to assess the quality of generated\ninstructions for scientific literature understanding. Specifically,\nthe quality is evaluated from five aspects: clarity, complexity,\ncorrectness, usefulness, and adaptability, assigning each instruc-\ntion a score from 0 (lowest quality) to 5 (highest quality). In-\nstructions with an average score of less than 4 are filtered out.\nWe show the quality statistics of synthetic instructions in Fig-\nure 5. The detailed recipe for instruction quality evaluation with\nconcrete examples is included in Appendix C.3.\nThrough instruction synthesis and quality control pipeline, we\nobtain SciLitIns , consisting of approximately 110,000 high-quality\nand diverse instructions for scientific literature understanding. With\nthese instructions, models\u2019 problem-solving abilities in this special-\nized field could be enhanced.\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nModels MMLU-s CMMLU-s Xiezhi-en-s Xiezhi-ch-s\n# Parameters < 15B\nChatGLM-6B 46.99 46.42 58.33 63.50\nMistral-7B 59.84 36.43 64.97 53.34\nQwen1.5-7B 57.09 73.74 65.87 71.60\nQwen2-7B 66.62 86.04 71.53 74.36\nLlama2-7B 39.87 28.33 42.40 36.40\nLlama3-8B 61.52 42.16 66.29 63.46\nLlama2-13B 49.06 32.29 58.30 42.65\nQwen1.5-14B 66.67 80.19 68.60 73.94\nSciLitLLM-7B 70.85 91.84 73.42 78.24\n# Parameters > 50B\nMixtral-8x7B 67.58 44.88 69.54 65.81\nQwen2-57B-A14B 73.79 89.65 70.35 73.73\nLlama2-70B 65.03 43.94 66.75 65.98\nLlama3-70B 76.43 66.41 71.36 73.28\nQwen1.5-72B 74.89 86.87 71.18 74.47\nQwen2-72B 80.86 92.31 72.41 75.03\nQwen1.5-110B 77.06 90.72 73.45 73.14\nSciLitLLM-72B (QLoRA) 82.31 93.08 74.23 76.93\nTable 2: Performance comparison of base models. Bold indi-\ncates the highest performance for LLMs under 15B param-\neters or above 50B parameters. SciLitLLM achieves leading\nperformance on all four scientific knowledge benchmarks.\n3.2.3 SFT Training Settings. Our SFT training dataset consists of\nthree parts: SciLitIns, SciRIFF [ 50] and Infinity-Instruct7, as shown\nin Table 1. Infinity-Instruct is a collection of more than twenty\nopen-source instructions datasets, covering various general do-\nmains. SciRIFF and SciLitIns contain specialized instructions for\nscientific literature understanding. We use full parameter training\nfor SciLitLLM-7B-Base and QLoRA [ 15] parameter-efficient training\nfor SciLitLLM-72B-Base. For both models, we train for one epoch\non Infinity-Instruct to cultivate their general instruction-following\nabilities, then for five epochs on SciLitIns and SciRIFF for scientific\nliterature understanding enhancement. The training is conducted\nwith a sequence length of 4,096, a maximum learning rate of 5\u00d710\u22126,\nand a cosine scheduler. The SFT training takes approximately 32\nhours on 32 A100 GPUs for the 7B and 100 hours for the 72B model,\nresulting in SciLitLLM-7B-Instruct and SciLitLLM-72B-Instruct.\n4 Experiments\nIn this section, we perform experiments to answer the following\nresearch questions:\n\u2022RQ1: How does SciLitLLM perform on scientific literature un-\nderstanding tasks?\n\u2022RQ2: Can CPT with domain-specific corpora aid in scientific\nknowledge injection?\n\u2022RQ3: Can SFT with synthetic instructions improve performance\non scientific literature understanding tasks?\n4.1 Experimental Setup\n4.1.1 Benchmarks. To evaluate the performance of LLMs regarding\nscientific knowledge base and specialized task-solving abilities, our\nbenchmarks include:\n\u2022CPT benchmarks. We evaluate the base models on three widely\nadopted benchmarks: MMLU [ 24], CMMLU [ 33], and Xiezhi [ 21].\n7https://huggingface.co/datasets/BAAI/Infinity-InstructSpecifically, we select the STEM subsets from these benchmarks\nto assess their scientific knowledge, which serves as the founda-\ntion for scientific literature understanding.\n\u2022SFT benchmarks. We evaluate the instruct models on scientific lit-\nerature understanding benchmarks: SciRIFF [ 50] and SciAssess [ 8].\nBrief descriptions of them are provided in Appendix D.\n4.1.2 Baselines. We test the following baselines:\n\u2022CPT baselines: We compare SciLitLLM-base against leading open-\nsource base models: ChatGLM [ 59], Llama3 [ 4], Llama2 [ 49],\nQwen2 [ 56], Qwen1.5 [ 48], Mistral-7B [ 28] and Mixtral-8x7B [ 29].\n\u2022SFT baselines: We benchmark leading instruction LLMs including\nGPT-4o [ 38], GPT-3.5 [ 7], Llama3 [ 4] and Qwen2 [ 56]. We also\nreport the performance of SciTulu-7B [ 50], which is a fine-tuned\nLlama2-7B [49] on SciRIFF.\n4.2 Performance Overview (RQ1)\n4.2.1 Base model performance. The performance comparison of\nbase models is shown in Table 2. SciLitLLM-base consistently out-\nperforms other general base models across four scientific bench-\nmarks. Specifically, compared with LLMs of less than 15 billion\nparameters, SciLitLLM-7B-Base shows an average accuracy im-\nprovement of 3.9% over Qwen2-7B. For LLMs with more than 50\nbillion parameters, SciLitLLM-72B-Base, with QLoRA training, out-\nperforms all other LLMs (without quantization) as large as 110\nbillion parameters. The results demonstrate the effectiveness of\nCPT on high-quality scientific corpora, paving the way to a special-\nized LLM for scientific literature understanding.\n4.2.2 Instruct model performance. As shown in Table 3, SciLitLLM-\n7B-Instruct achieves the highest performance in 4 out of 5 domains\non SciAssess, outperforming the second-best model by 3.6%. No-\ntably, on SciRIFF, it surpasses baseline models by a substantial mar-\ngin of 10.1%. Additionally, SciLitLLM-72B, trained using QLoRA,\nshows a 1.7% and 0.9% performance improvement over Qwen2-72B\non SciAssess and SciRIFF, respectively.\nDetailed model performance on SciAssess is presented in Table 7,\nwhere SciLitLLM-7B and SciLitLLM-72B both lead in 12 and 13 out\nof 29 sub-tasks. Specifically, SciLitLLM-7B excels in tasks such as\ntable extraction and molecule generation, likely benefiting from the\ncomprehensive task coverage in our synthetic instruction dataset,\nSciLitIns. On SciRIFF, SciLitLLM-7B/SciLitLLM-72B ranks first in\n8/6 out of 11 evaluations8.\n4.3 Ablation Study (RQ2 & RQ3)\nWe conducted ablation experiments on three key components in\nour pipeline: the CPT stage, the SFT data recipe, and the instruction\nquality filter, to demonstrate their effectiveness. It is important to\nnote that all ablation experiments were performed on SciLitLLM-7B\ndue to budget constraints.\n4.3.1 Scientific knowledge injection via CPT (RQ2). We investigate\nthe contribution of the CPT stage for SciLitLLM. We compare the\nthree variants: (1) Qwen2-7B-Instruct: official instruct-model check-\npoint; (2) Qwen2-7B-base + SFT: applying our SFT stage directly\n8In SciRIFF, the Qasper and SciFact tasks have two different evaluation methods and\nthus two results.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Sihang Li et al.\nDatasetDomain/\nTask# Parameter ~7B # Parameter ~70B API\nSciTulu-7B Mistral-7B Llama3-8B Qwen2-7B SciLitLLM-7B Llama3-70B Qwen2-72B SciLitLLM-72B (QLoRA) GPT3.5 GPT4o\nSciAssessFundSci 32.3 48.3 58.5 70.3 74.8 70.9 77.1 78.4 62.2 76.7\nAlloyMat 23.9 28.0 32.9 32.8 35.6 44.9 42.7 49.1 32.0 52.1\nBiomed 67.8 76.0 77.4 80.8 79.6 79.6 81.0 79.6 78.0 82.3\nDrugDisc 25.4 30.2 32.0 31.7 33.2 41.5 35.5 41.8 31.0 43.4\nOrgMat 16.7 20.6 24.5 28.3 38.9 41.5 52.7 48.6 24.4 62.7\nMean 33.2 40.6 45.0 48.8 52.4 55.7 57.8 59.5 45.5 63.4\nSciRIFFBioASQ 37.5 43.9 44.7 40.7 51.0 46.3 43.6 50.7 47.3 46.7\nBioR 55.7 48.2 45.3 44.3 74.0 59.9 59.1 63.0 53.9 61.0\nDiscMT 61.5 44.6 58.7 59.9 77.4 71.9 73.5 73.5 67.9 78.3\nEI 11.6 17.1 14.7 14.4 22.3 22.0 24.1 23.1 19.2 24.7\nMC 34.6 47.0 49.5 51.6 68.0 59.9 57.3 70.5 47.8 58.7\nMuP 72.1 93.4 90.7 96.6 76.8 96.4 97.8 77.9 76.8 86.9\nQasper 54.2/38.6 58.6/39.4 58.2/41.9 56.8/34.5 58.4/ 56.9 25.0/19.4 63.3/47.2 60.5/ 54.1 54.7/39.8 67.8/50.5\nSciERC 35.6 30.2 19.9 27.5 39.9 35.2 34.1 46.9 28.6 42.2\nSciFact 66.0/49.2 68.5/51.3 64.6/51.7 65.2/44.3 68.5/ 59.7 85.1 /67.3 82.3/65.9 75.2/60.6 69.7/53.3 84.3/68.7\nMean 47.0 49.3 49.1 48.7 59.4 53.5 58.9 59.7 50.8 60.9\nTable 3: Model performances on scientific literature understanding benchmarks: SciAssess and SciRIFF. SciLitLLM-7B and\nSciLitLLM-72B achieve leading performance compared with models of similar scales. The best-performing models in the 7B\nand 70B scales are highlighted in bold. Results for SciTulu-7B, GPT-3.5, and GPT-4o on SciRIFF are taken from its original\npapers, while all other results are generated by our experiments.\nModel SciAssess SciRIFF\nQwen2-7B-Instruct 48.8 48.7\nQwen2-7B-Base + SFT 48.1 51.6\nQwen2-7B-Base + CPT + SFT 52.4 59.4\nTable 4: Ablation study of the CPT stage. The results demon-\nstrate the effectiveness of the CPT stage in improving per-\nformance on scientific literature understanding.\nSFT Dataset SciAssess SciRIFF\nInfinity-Instruct 44.5 44.7\nInfinity-Instruct + SciRIFF 42.2 53.9\nInfinity-Instruct + SciRIFF + SciLitIns 52.4 59.4\nTable 5: Ablation study of SFT data recipes.\nto Qwen2-7B-base without CPT; (2) Qwen2-7B-base + CPT + SFT:\nSciLitLLM-7B-Instruct.\nAs shown in Table 4, applying SFT alone to the Qwen2-7B-Base\nmodel does not lead to clear performance gains on SciAssess (-\n0.7%) and yields only a modest improvement on SciRIFF (+2.9%).\nIn contrast, incorporating both CPT and SFT results in substan-\ntial performance enhancements: a 3.6% improvement on SciAssess\nand a 10.7% gain on SciRIFF. These results demonstrate that the\nCPT is crucial for effectively injecting scientific knowledge and\nsignificantly enhancing LLM performance on scientific literature\nunderstanding tasks.\n4.3.2 Influence of SFT Data Recipes (RQ3). We explore the influ-\nence of each ingredient in SFT data recipes. We incrementally add\nthree datasets to the SFT training set: Infinity-Instruct, SciRIFF, and\nSciLitIns. As shown in Table 5, using only the Infinity-Instruct re-\nsults in the lowest performance on both SciAssess (44.5%) and\nSciRIFF (44.7%). This indicates that fine-tuning LLMs on general\ninstructions alone is insufficient for scientific literature understand-\ning, likely because Infinity-Instruct lacks specialized contents.Dataset SciAssess SciRIFF\nSciLitIns w/o filtering 51.1 56.2\nSciLitIns w/ filtering 52.4 59.4\nTable 6: Ablation study of SFT instruction quality filtering.\nAdding SciRIFF to Infinity-Instruct improves performance on\nSciRIFF significantly but decreases performance on SciAssess. This\ndiscrepancy may be due to the disjoint coverage of scientific do-\nmains between SciRIFF and SciAssess. Finally, including SciLitIns along\nwith Infinity-Instruct and SciRIFF boosts performance on both\nbenchmarks, with SciAssess at 52.4% and SciRIFF at 59.4%. This\ndemonstrates that including SciLitIns that covers less-represented\nscientific domains and tasks is beneficial for enhancing model per-\nformance in scientific literature understanding.\n4.3.3 Influence of Instruction Quality Filter. We conduct an ablation\nstudy to assess the impact of quality filter for synthetic instructions\nby varying whether the dataset SciLitIns was filtered. As discussed\nin Section 3.2.2, this filter removes low-quality instructions evalu-\nated from five key aspects. Table 6 shows that applying the filter\nsignificantly improved the performance of SciLitLLM-7B on Sci-\nAssess (+1.3%) and SciRIFF (+3.2%). This demonstrates that SFT\nquality filtering process effectively selects high-value educational\ninstructions, thereby boosting the performance of SciLitLLMon\nscientific literature understanding.\n5 Limitations\nDespite the promising results achieved by SciLitLLM, there are\nseveral limitations that should be acknowledged:\n\u2022Insufficient data volume. Compared with existing pre-training\ndatasets [ 4,47,56], the amount of data used for CPT is not satisfy-\ning. Future work should consider incorporating a larger scientific\ncorpus, potentially including scientific blogs or purely synthetic\ndata, to enhance the model\u2019s scientific knowledge base and over-\nall performance.\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n\u2022Lack of reasoning enhancement. The current pipeline does not ex-\nplore advanced reasoning techniques such as Chain-of-Thought [ 53]\nor Tree-of-Thought [ 58] in the data construction or model in-\nference stages. Investigating these methods could potentially\nimprove the model\u2019s inference capabilities and overall perfor-\nmance.\n\u2022Lack of preference alignment. Due to a limited financial budget,\nthe model lacks Reinforcement Learning from Human Feedback\n(RLHF) [ 39]. RLHF has shown significant improvements in align-\ning models with human preferences and ensuring more reliable\noutputs. Implementing RLHF in future iterations could further\nenhance the model\u2019s reliability.\nAddressing these limitations in future research will be crucial de-\nveloping a more robust and capable LLM specialized in scientific\nliterature understanding.\n6 Conclusion and Future Works\nIn this paper, we introduce SciLitLLM , a specialized model for\nscientific literature understanding. It is initialized with a general\nbase model \u2013 Qwen2 [ 56], and trained through a sequential pipeline\nof continual pre-training (CPT) and supervised fine-tuning (SFT).\nFor effective scientific knowledge injection during CPT, we propose\nmodel-based format and grammar correction method, along with\ntext quality filtering measures. To ensure high-quality and diverse\ninstructions during SFT, we devise instruction synthesis and quality\ncontrol approaches. Our experiments on widely-used benchmarks\ndemonstrate the effectiveness of this pipeline in adapting a general\nmodel to the field of scientific literature understanding. Specifically,\nSciLitLLM-7B achieves a 3.6% improvement on the SciAssess [ 8]\nand a 10.1% improvement on the SciRIFF [ 50] compared to lead-\ning models with fewer than 10 billion parameters. SciLitLLM-72B,\ntrained with QLoRA, also surpasses baseline open-source LLMs. We\nnote that this pipeline could be easily adapted to other specialized\ndomains, particularly those lacking adequate open-source corpora\nand high-quality instruction sets.\nOur future work will focus on expanding the diversity and quality\nof the training data, as well as exploring more efficient methods for\ndomain-specific knowledge injection and high-quality instruction\ngeneration. Moreover, we plan to expand our pipeline to include\nthe RLHF [ 39] stage for better human preference alignment and\nenhanced safety.\nReferences\n[1]Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed\nAwadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\nrat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Mar-\ntin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul\nChopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan\nIter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng\nHao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann,\nNikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R.\nLee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush\nMadan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra,\nDaniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmi-\nlac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied,\nAdil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia\nSong, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte,\nMichael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan\nYu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang,\nYue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: AHighly Capable Language Model Locally on Your Phone. CoRR abs/2404.14219\n(2024).\n[2]Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya,\nAnnika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan M. Co-\nhen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi\nDong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona\nGhosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jin-\ning Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu,\nSadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu,\nEileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki,\nMiguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan,\nSean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi,\nGuruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary,\nKrzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar,\nVasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall,\nPavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyan-\nskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian,\nShengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You,\nJiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, and Chen\nZhu. 2024. Nemotron-4 340B Technical Report. CoRR abs/2406.11704 (2024).\nhttps://doi.org/10.48550/arXiv.2406.11704\n[3]Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The Impact\nof Large Language Models on Scientific Discovery: a Preliminary Study using\nGPT-4. CoRR abs/2311.07361 (2023).\n[4]AI@Meta. 2024. Llama 3 Model Card. (2024). https://github.com/meta-llama/\nllama3/blob/main/MODEL_CARD.md\n[5]Lozhkov Anton, Ben Allal Loubna, von Werra Leandro, and Wolf Thomas. 2024.\nFineWeb-Edu . https://doi.org/10.57967/hf/2497\n[6]Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language\nModel for Scientific Text. In EMNLP/IJCNLP (1) . Association for Computational\nLinguistics, 3613\u20133618.\n[7]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In\nNeurIPS .\n[8]Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin\nWang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang,\nJiankun Wang, Yuqi Yin, Yaqi Li, Linfeng Zhang, and Guolin Ke. 2024. SciAssess:\nBenchmarking LLM Proficiency in Scientific Literature Analysis. CoRR (2024).\nhttps://doi.org/10.48550/arXiv.2403.01976\n[9]Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu\nWei. 2024. Instruction Pre-Training: Language Models are Supervised Multitask\nLearners. arXiv preprint arXiv:2406.14491 (2024).\n[10] Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023. Adapting Large Language\nModels via Reading Comprehension. CoRR abs/2309.09530 (2023).\n[11] David Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be\nan Alternative to Human Evaluations?. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and\nNaoaki Okazaki (Eds.). Association for Computational Linguistics, 15607\u201315631.\nhttps://doi.org/10.18653/v1/2023.acl-long.870\n[12] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero,\nJan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia L\u00f6ffler, Sophie-\nCaroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al .2023. The\nfuture landscape of large language models in medicine. Communications medicine\n3, 1 (2023), 141.\n[13] Together Computer. 2023. RedPajama: an Open Dataset for Training Large Lan-\nguage Models . https://github.com/togethercomputer/RedPajama-Data\n[14] Jiaxi Cui, Munan Ning, Zongjian Li, Bohua Chen, Yang Yan, Hao Li, Bin Ling,\nYonghong Tian, and Li Yuan. 2024. Chatlaw: A Multi-Agent Collaborative Legal\nAssistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language\nModel. arXiv:2306.16092 [cs.CL] https://arxiv.org/abs/2306.16092\n[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.\nQLoRA: Efficient Finetuning of Quantized LLMs. In NeurIPS .\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT (1) . Association for Computational Linguistics, 4171\u20134186.\n[17] Ronen Eldan and Yuanzhi Li. 2023. TinyStories: How Small Can Language\nModels Be and Still Speak Coherent English? CoRR abs/2305.07759 (2023). https:\n//doi.org/10.48550/arXiv.2305.07759\n[18] Alexander Erdmann, David Joseph Wrisley, Benjamin Allen, Christopher Brown,\nSophie Cohen-Bod\u00e9n\u00e8s, Micha Elsner, Yukun Feng, Brian Joseph, B\u00e9atrice Joyeux-\nPrunel, and Marie-Catherine de Marneffe. 2019. Practical, Efficient, and Customiz-\nable Active Learning for Named Entity Recognition in the Digital Humanities. In\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Sihang Li et al.\nNAACL-HLT (1) . Association for Computational Linguistics, 2223\u20132234.\n[19] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen,\nXiaohui Fan, and Huajun Chen. 2023. Mol-Instructions: A Large-Scale Biomolec-\nular Instruction Dataset for Large Language Models. CoRR abs/2306.08018 (2023).\nhttps://doi.org/10.48550/arXiv.2306.08018\n[20] Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming\nQin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. 2024. SciKnowEval:\nEvaluating Multi-level Scientific Knowledge of Large Language Models. CoRR\n(2024). https://doi.org/10.48550/arXiv.2406.09098\n[21] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin\nZhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, Qianyu He, Rui Xu,\nWenhao Huang, Jingping Liu, Zili Wang, Shusen Wang, Weiguo Zheng, Hongwei\nFeng, and Yanghua Xiao. 2024. Xiezhi: An Ever-Updating Benchmark for Holistic\nDomain Knowledge Evaluation. In AAAI . AAAI Press, 18099\u201318107. https:\n//doi.org/10.1609/aaai.v38i16.29767\n[22] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\nOlli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien\nBubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.\nTextbooks Are All You Need. CoRR abs/2306.11644 (2023).\n[23] Kshitij Gupta, Benjamin Th\u00e9rien, Adam Ibrahim, Mats L. Richter, Quentin An-\nthony, Eugene Belilovsky, Irina Rish, and Timoth\u00e9e Lesort. 2023. Continual\nPre-Training of Large Language Models: How to (re)warm your model? CoRR\nabs/2308.04014 (2023).\n[24] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Un-\nderstanding. In ICLR . https://openreview.net/forum?id=d7KBjmI3GmQ\n[25] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-\nOptimal Large Language Models. CoRR abs/2203.15556 (2022).\n[26] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language\nModels. CoRR abs/2106.09685 (2021). https://arxiv.org/abs/2106.09685\n[27] Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen,\nZirui Wu, and Yansong Feng. 2023. Lawyer LLaMA Technical Report. CoRR\nabs/2305.15062 (2023). https://doi.org/10.48550/arXiv.2305.15062\n[28] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\nvendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023. Mistral 7B. CoRR abs/2310.06825 (2023). https:\n//doi.org/10.48550/arXiv.2310.06825\n[29] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou\nHanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,\nL\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep\nSubramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet,\nThibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2024.\nMixtral of Experts. CoRR abs/2401.04088 (2024). https://doi.org/10.48550/arXiv.\n2401.04088\n[30] Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei,\nAndrew O. Arnold, and Xiang Ren. 2022. Lifelong Pretraining: Continually\nAdapting Language Models to Emerging Corpora. In NAACL-HLT . Association\nfor Computational Linguistics, 4764\u20134780.\n[31] Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.\n2023. Continual Pre-training of Language Models. In ICLR . OpenReview.net.\n[32] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nMemory Management for Large Language Model Serving with PagedAttention.\nInSOSP . ACM, 611\u2013626.\n[33] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan\nDuan, and Timothy Baldwin. 2023. CMMLU: Measuring massive multitask\nlanguage understanding in Chinese. CoRR abs/2306.09212 (2023). https://doi.\norg/10.48550/arXiv.2306.09212\n[34] Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji\nKawaguchi, Tat-Seng Chua, and Qi Tian. 2024. Towards 3D Molecule-Text\nInterpretation in Language Models. CoRR abs/2401.13923 (2024).\n[35] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar,\nand Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report.\nCoRR abs/2309.05463 (2023).\n[36] Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang\nWang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao,\nAmit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng\nChen, Chris White, Quanquan Gu, Carl Yang, and Liang Zhao. 2023. Beyond\nOne-Model-Fits-All: A Survey of Domain Specialization for Large LanguageModels. CoRR abs/2305.18703 (2023). https://doi.org/10.48550/ARXIV.2305.18703\narXiv:2305.18703\n[37] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. 2023.\nAn Empirical Investigation of the Role of Pre-training in Lifelong Learning. J.\nMach. Learn. Res. 24 (2023), 214:1\u2013214:50.\n[38] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). https:\n//doi.org/10.48550/arXiv.2303.08774\n[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\nAmanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow instructions with hu-\nman feedback. In NeurIPS . http://papers.nips.cc/paper_files/paper/2022/hash/\nb1efde53be364a73914f58805a001731-Abstract-Conference.html\n[40] Hang Qiu, Krishna Chintalapudi, and Ramesh Govindan. 2023. MCAL: Minimum\nCost Human-Machine Active Labeling. In ICLR . OpenReview.net.\n[41] Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao\nMa, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, et al .2024. D-CPT\nLaw: Domain-specific Continual Pre-Training Scaling Law for Large Language\nModels. arXiv preprint arXiv:2406.01375 (2024).\n[42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al.2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach.\nLearn. Res. 21 (2020), 140:1\u2013140:67.\n[44] Amanpreet Singh, Mike D\u2019Arcy, Arman Cohan, Doug Downey, and Sergey Feld-\nman. 2023. SciRepEval: A Multi-Format Benchmark for Scientific Document\nRepresentations. In EMNLP . 5548\u20135566. https://doi.org/10.18653/v1/2023.emnlp-\nmain.338\n[45] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won\nChung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nPerry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Sch\u00e4rli,\nAakanksha Chowdhery, Philip Andrew Mansfield, Blaise Ag\u00fcera y Arcas, Dale R.\nWebster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad\nTomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan\nKarthikesalingam, and Vivek Natarajan. 2022. Large Language Models Encode\nClinical Knowledge. CoRR abs/2212.13138 (2022). https://doi.org/10.48550/arXiv.\n2212.13138\n[46] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and\nHaifeng Wang. 2020. ERNIE 2.0: A Continual Pre-Training Framework for Lan-\nguage Understanding. In AAAI . AAAI Press, 8968\u20138975.\n[47] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony\nHartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.\n2022. Galactica: A Large Language Model for Science. CoRR abs/2211.09085\n(2022). https://doi.org/10.48550/arXiv.2211.09085\n[48] Qwen Team. 2024. Introducing Qwen1.5. https://qwenlm.github.io/blog/qwen1.\n5/\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-\nple. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR\nabs/2302.13971 (2023).\n[50] David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan\nBarzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey,\nHannaneh Hajishirzi, and Arman Cohan. 2024. SciRIFF: A Resource to En-\nhance Language Model Instruction-Following over Scientific Literature. CoRR\nabs/2406.07835 (2024). https://doi.org/10.48550/arXiv.2406.07835\n[51] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models\nare zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n[52] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned Language\nModels are Zero-Shot Learners. In ICLR . OpenReview.net.\n[53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\nXia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. In NeurIPS .\n[54] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian\nGehrmann, Prabhanjan Kambadur, David S. Rosenberg, and Gideon Mann. 2023.\nBloombergGPT: A Large Language Model for Finance. CoRR abs/2303.17564\n(2023). https://doi.org/10.48550/ARXIV.2303.17564 arXiv:2303.17564\n[55] Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, and Gho-\nlamreza Haffari. 2022. Pretrained Language Model in Continual Learning: A\nComparative Study. In ICLR . OpenReview.net.\n[56] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-\npeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei,\nHuan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nJianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang\nLin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue,\nNa Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie\nWang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge,\nXiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xu-\nancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei\nChu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024.\nQwen2 Technical Report. https://arxiv.org/abs/2407.10671\n[57] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023. FinGPT: Open-\nSource Financial Large Language Models. CoRR abs/2306.06031 (2023). https:\n//doi.org/10.48550/arXiv.2306.06031\n[58] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and\nKarthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with\nLarge Language Models. In NeurIPS .\n[59] Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu\nFeng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang,\nJiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu,\nLucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu,\nShuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi\nZhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu,\nXinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin\nNiu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang,\nZhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024. ChatGLM: A\nFamily of Large Language Models from GLM-130B to GLM-4 All Tools. CoRR\nabs/2406.12793 (2024). https://doi.org/10.48550/arXiv.2406.12793\n[60] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022. A Deep-learning\nSystem Bridging Molecule Structure and Biomedical Text with Comprehension\nComparable to Human Professionals. Nature communications 13, 862 (2022).\n[61] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang,\nYisong Yue, Yuxiao Dong, and Jie Tang. 2024. SciGLM: Training Scientific Lan-\nguage Models with Self-Reflective Instruction Annotation and Tuning. CoRR\nabs/2401.07950 (2024). https://doi.org/10.48550/arXiv.2401.07950\n[62] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li,\nWeiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su, Hansen\nZhong, Yuqiang Li, and Wanli Ouyang. 2024. ChemLLM: A Chemical Large\nLanguage Model. CoRR abs/2402.06852 (2024). https://doi.org/10.48550/arXiv.\n2402.06852\n[63] Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu,\nZiyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, and Qiaozhu Mei.\n2024. MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific\nWorkflows. CoRR abs/2406.06357 (2024). https://doi.org/10.48550/arXiv.2406.\n06357\n[64] Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May,\nGeoffrey I. Webb, and Shirui Pan. 2023. Large Language Models for Scientific\nSynthesis, Inference and Explanation. CoRR abs/2310.07984 (2023).\nA Format & Grammar Correction Examples\nRaw text parsed by PyPDF2\nHighly p e n e t r a t i n g radiation, such as \ud835\udefe-rays or fast electorns, deposits ener gy\nthroughout the solid t a r g e t material. Gas production occurs w i t h i n the solid phase and\nmust d i f f u s e to the surface to be observed. The a p p a r e n t yield of H\n2 can depend on the radiolysis\npro c e d u r e or the particle size if some of the gas remains in the solid. Experiments have\nshown\nthat the apparent y i e l d of H 2 can vary by a f a c t o r of 3 in the r a d i o l y s i s of polyethylene\nspheres\nof 7 to 2100 cm2/g (about 9 to 0.03 mm) [12]. The e f f e c t s of gas trapping and diffusion are\nnot understood in the c o n t e x t of waste storage. Extremely h i g h dose rates in the p r o c e s\ns i n g of certain materials may lead to bubble formation, which could a l t e r product quality.\nThe y i e l d of H\n2 in the r a d i o l y s i s of polymers w i t h \ud835\udefe-rays is well known for several\ntypes of p o l y m e r s [2].\nFormat & Grammar Correction Prompt\nI have extracted the following raw text from a PDF, but the extraction process has introduced\nmany formatting issues such as unnecessary line breaks, extra spaces, and other artifacts that\ndisrupt the text flow. Could you please help me correct these formatting issues and provide a\nclean, readable version of the text? Respond with the Corrected Version only.\nRaw Text:\n{RawText}\nStart your response with \"Here is the corrected version of the text:\".Text after Format & Grammar Correction\nHighly penetrating radiation, such as \ud835\udefe-rays or fast electrons, deposits energy throughout the\nsolid target material. Gas production occurs within the solid phase and must diffuse to the\nsurface to be observed. The apparent yield of H2 can depend on the radiolysis procedure or the\nparticle size if some of the gas remains in the solid. Experiments have shown that the apparent\nyield of H2 can vary by a factor of 3 in the radiolysis of polyethylene spheres of 7 to 2100 cm2/g\n(about 9 to 0.03 mm) [12]. The effects of gas trapping and diffusion are not understood in the\ncontext of waste storage. Extremely high dose rates in the processing of certain materials may\nlead to bubble formation, which could alter product quality.\nThe yield of H2 in the radiolysis of polymers with \ud835\udefe-rays is well known for several types of\npolymers [2].\nB CPT Quality Filter\nWe randomly select 50k samples from our 700k CPT data. These\nselected samples are then scored using the Llama3-70B model. The\nprompt utilized for this scoring process is as follows:\nPrompt for CPT Data Quality Labelling\nBelow is an extract from a textbook. Evaluate whether the text has a high educational value and\ncould be useful in an educational setting for teaching from primary school to grade school\nlevels using the additive 5-point scoring system described below. Points are accumulated based\non the satisfaction of each criterion:\n- Add 1 point if the extract provides some basic information relevant to educational topics, even\nif it includes some irrelevant or non-academic content like advertisements and promotional\nmaterial.\n- Add another point if the extract addresses certain elements pertinent to education but does not\nalign closely with educational standards. It might mix educational content with non-educational\nmaterial, offering a superficial overview of potentially useful topics, or presenting information\nin a disorganized manner and incoherent writing style.\n- Award a third point if the extract is appropriate for educational use and introduces key concepts\nrelevant to school curricula. It is coherent though it may not be comprehensive or could include\nsome extraneous information. It may resemble an introductory section of a textbook or a basic\ntutorial that is suitable for learning but has notable limitations like treating concepts that are\ntoo complex for grade school students.\n- Grant a fourth point if the extract is highly relevant and beneficial for educational purposes\nfor a level not higher than grade school, exhibiting a clear and consistent writing style. It could\nbe similar to a chapter from a textbook or a tutorial, offering substantial educational content,\nincluding exercises and solutions, with minimal irrelevant information, and the concepts aren\u2019t\ntoo advanced for grade school students. The content is coherent, focused, and valuable for\nstructured learning.\n- Bestow a fifth point if the extract is outstanding in its educational value, and perfectly suited\nfor teaching either at primary school or grade school. It follows detailed reasoning, the writing\nstyle is easy to follow, and offers profound and thorough insights into the subject matter, devoid\nof any non-educational or complex content.\nAfter examining the extract:\n- Briefly justify your total score, up to 100 words.\n- Conclude with the score using the format: \"Educational score: <total points>\"\nWe train a Scientific Texts Quality Classifier on these labeled\ndata samples. The classifier is a 109M BERT [ 16] classifier, fine-\ntuned from the checkpoint of fineweb-edu-classifier [ 5]. The model\nis trained for 20 epochs with a learning rate of 0.001 and a batch\nsize of 1024. Ninety percent of the 50K samples are used as the\ntraining set, and the rest 10% are used as the validation set. The\ntraining process costs approximately 50 minutes on 4 A100 GPUs.\nWe select the checkpoint from the epoch that yields the highest\nvalidation micro F1 score as our final checkpoint.\nDuring Inference, we set batch size to 2048, and beam number\nto 1. The inference process costs 90 minutes on 4 A100 GPUs. We\nutilize the generated to filter out 25% data with the lowest quality.\nThe distribution of the scores is demonstrated in Figure 6. The\nfiltered-out 25% data are marked gray , while the remaining 75%\nCPT data are marked orange .\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Sihang Li et al.\n0 1 2 3 4 5\nScoreFrequency\nFigure 6: Score distribution of the CPT Data\nC SFT Details\nC.1 Instruction Generation Pipeline\nIn SciLitIns, we focus on generating instructions for three less-\nrepresented domains (materials science, medicine, and drug discov-\nery) and five question types:\n\u2022Table Extraction: Table Extraction tasks evaluate a model\u2019s\nproficiency in extracting, summarizing, and structuring data\nfrom an article into a table format.\n\u2022Entity Extraction: Entity Extraction tasks are designed to\nevaluate a model\u2019s ability to extract specific information,\nsuch as entities or relationships, from the text.\n\u2022Molecule Translation: Molecule Translation tasks evaluate\na model\u2019s ability to translate molecules between different\nSMILES formats.\n\u2022Molecule Extraction: Molecule Extraction tasks ask a model\nto extract an appropriate molecule from a scientific para-\ngraph that contains multiple molecules.\n\u2022Multiple Choice and True-or-False: Multiple Choice and\nTrue-or-False questions assess a model\u2019s ability to select the\ncorrect answer from a set of options, testing its knowledge\nand reasoning on both simple and complex scenarios.\nFor each of the three scientific domains, we collect a set of high-\nimpact research papers and construct a word frequency table. To\ngenerate a question in a given domain, we sample 20 keywords\nfrom the corresponding word table and insert them into the prompt\nfor that question. To ensure fair representations of less frequent\nkeywords, we use random sampling with a temperature setting of 3.\nWe release our code, prompt templates, and word frequency tables.\nBelow is an example of generating a table extraction question:Prompt for Generating a Table Extraction Question\nI need synthetic training data for training a machine learning model that extracts tables from\ntext correctly. The data should be formatted in JSON, with each entry containing \"text\" and\n\"answer\" attributes. You should generate a paragraph that includes the keywords:\n{{keywords}}.\nThe \"text\" part must contain enough information for the table to be extracted! In \"text\" part, You\nmust you include a table description in latex format.\nSpecial notice for the table content:\nYou should generate a table that has complicated numbers and characters, include non-standard\ncharacters, and have a variety of values. Make sure the value you generated do not follow simple\npatterns, for example, never include deplicate values or values with constant interval in columns.\nYour answer should contain as much details as possible. You should only generate one JSON.\nThe value for the two attributes should be two string. Use {{ and }} to warp your output. Pay\nattention to the escape characters in the latex format. Remember to put a comma at the end of\nthe first string. Never use a json block to wrap your output. Here is the format for your output:\n{\n\"text\": \"Your paragraph here, remember to include a table in latex format\",\n\"answer\": \"Your answer table here\"\n}\nNow start your answer:\nC.2 Instruction Deduplication\nThe generated synthetic data may contain similar questions or\nidentical answers. To eliminate redundancy, we implement a fuzzy\ndeduplication process using the Levenshtein distance to calculate\nthe similarity score between question-answer pairs. Specifically,\nfor two pairs(\ud835\udc5e1, \ud835\udc4e1)and(\ud835\udc5e2, \ud835\udc4e2), their textual similarity is de-\nfined as(1\u2212lev(\ud835\udc5e1, \ud835\udc5e2))(1\u2212lev(\ud835\udc4e1, \ud835\udc4e2)), where lev(\u00b7,\u00b7)denotes\nthe Levenshtein distance. Due to significant differences between\ntexts from different question types, we compute similarity matrices\nseparately for each type. We then use a disjoint-set data structure\nto merge highly similar data points. We use this process to remove\napproximately 5% to 10% of duplicated data for each question type.\nC.3 Quality Assessment of Generated SFT\nInstructions\nIn section 3.2.2, we sample 10k instruction pairs from SciLitIns and\nevaluate them by Llama-3-70B using the below prompt.\nSFT Evaluation Prompt\nYou are a helpful and precise assistant for checking the quality of instruction-tuning data for large\nlanguage models. Your task is to evaluate the given instruction using the criterions described\nbelow.\n- Clarity: The sample should be clear, specific, and unambiguous, providing a well-defined task\nfor the model to perform.\n- Complexity: The sample should be advanced complexity that necessitate a high level of com-\nprehension and cognitive processing, challenging the language model significantly.\n- Correctness: The sample is impeccably written, with flawless grammar, syntax, and structure,\ndemonstrating exceptional clarity and professionalism.\n- Usefulness: The sample should be highly useful, and contribute to expanding the model\u2019s\nknowledge base.\n- Adaptability: The sample could be adapted to different contexts or use cases, showing some\nflexibility.\nAfter examining the instruction-response pair:\n- Briefly justify your scores with a paragraph in the field \"Explanation\", up to 500 words.\n- For each point of criterion above, assign a score from 1 to 5.\n- You should only provide the rest of your answer in a structured format as shown below, and\nmake sure your response can be directly parsed by computer programs.\nBelow is a template for your response:\nExplanation: <string, your explanations to the scores>\n====================\n{\n\"Clarity\": <int, complexity_score >,\n\"Complexity\": <int, complexity_score >,\n\"Correctness\": <int, quality_score >,\n\"Usefulness\": <int, usefulness_score >,\n\"Adaptability\": <int, adaptability_score >,\n\"Total\": <int, total_score >\n}\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nBelow is an example of SciLitIns, which will be sent to Llama-3-\n70B for evaluation.\nAn Example in SciLitIns (sharegpt format)\n1 \" conversations \":\n2 [\n3 {\n4 \" from \": \" human \",\n5 \" value \": \" Extract the table in the following paragraph .\n6 In recent studies regarding material science , the crmatrix\n7 has shown significant importance in understanding\nfracture behaviors .\n8 Alternatively , many researchers have proposed a series of\nexperiments to analyze these phenomena .\n9 Even though the process is delicate , the enthusiasm for\ndiscovering new findings is understandable . The\nfollowing Table 1 presents a schematic illustration\nof the enthalpy changes observed during the\ndifferent fracture phases . These studies are\nindispensable for advancing our knowledge of the\nmaterial 's resilience .\\n\\ nIn addition , in certain\ncases , pronounced errors in measurement can occur ,\nbut the references provided can guide through the\ncorrect methodology . Prime among the tools used is\nthe differential scanning calorimetry which helps\nmeasure these enthalpy changes accurately .\\n\\n\n\\ begin { table }[h !]\\ n\\ centering \\n\\ begin { tabular }\n{|c|c|c|c |}\\ n\\ hline \\ nMaterial & Initial Enthalpy\n(kJ/ mol ) & Final Enthalpy (kJ/ mol ) & Fracture Phase\n\\\\\\ n\\ hline \\ nMaterial A & 25.4 & 47.8 & Brittle\n\\\\\\ n\\ hline \\ nMaterial B & 22.3 & 45.0 & Ductile\n\\\\\\ n\\ hline \\ nMaterial C & 28.9 & 50.2 & Semi - brittle\n\\\\\\ n\\ hline \\n\\ end { tabular }\\n\\ caption { Enthalpy changes\nduring fracture\nphases }\\n\\ label { table : enthalpy }\\n\\ end { table } \\n\\ nThe\nprocess of accurately measuring these values is\ncrucial for the material selection in various\nengineering applications , making it an indispensable\nresearch area .\"\n10 },\n11 {\n12 \" from \": \" gpt \",\n13 \" value \": \" Material , Initial Enthalpy (kJ/ mol ),Final\nEnthalpy\n14 (kJ/ mol ),Fracture Phase \\ nMaterial\nA ,25.4 ,47.8 , Brittle \\ nMaterial\nB ,22.3 ,45.0 , Ductile \\ nMaterial\nC ,28.9 ,50.2 , Semi - brittle \"\n15 }\n16 ]\nBelow is the evaluation by Llama-3-70B on the example from SciL-\nitIns. Each criterion is scored out of 5 points, making the total pos-\nsible points 25. The total score is then averaged, resulting in an\noverall score out of 5.\nEvaluation by Llama-3-70B\nExplanation:\nThe instruction is clear and specific, asking the model to extract a table from a given paragraph.\nThe paragraph itself is well-written, with proper grammar and syntax, making it easy to un-\nderstand. The table is also well-structured and provides valuable information about enthalpy\nchanges during fracture phases. The task requires a good understanding of the text and the\nability to identify the table. However, the complexity of the task is relatively low, as it only\ninvolves extracting a table, and the context is not particularly nuanced or ambiguous. The task is\nuseful for advancing knowledge in material science, and the table could be adapted to different\ncontexts or use cases.\n====================\n\"Clarity\": 5,\n\"Complexity\": 2,\n\"Correctness\": 5,\n\"Usefulness\": 4,\n\"Adaptability\": 4,\n\"Total\": 20D Benchmark Details\nTo the best of our knowledge, there are two commonly-adopted\ndatasets for scientific literature understanding:\n\u2022SciRIFF [ 50] evaluates essential scientific literature understand-\ning capabilities, including information extraction, summarization,\nquestion answering, claim verification, and classification. Data\npoints in SciRIFF are notable for their long input contexts and\ncomplicated structured outputs. The Qasper and SciFact tasks\nhave two different evaluation methods and thus two results. We\nnote that SciRIFF contains a separate training set used in the SFT\nstage in our study.\n\u2022SciAssess [ 8] features an end-to-end benchmark of understanding\nPDF content. It includes 29 tasks from five scientific domains: fun-\ndamental science, alloy materials, biomedicine, drug discovery,\nand organic materials. SciAssess is used exclusively for testing\nin our evaluation.\nOverall, SciRIFF provides basic benchmarks for comprehending\nshort scientific segments and various instructions, while SciAssess\npresents more challenging tasks involving longer contexts from\nraw PDFs.\nE Detailed Performance on SciAssess\nThe detailed results on each tasks in SciAssess are shown in Table 7.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Sihang Li et al.\nDomain Task SciTulu-7B Mistral-7B Llama3-8B Qwen2-7B SciLitLLM-7B Llama3-70B Qwen2-72B SciLitLLM-72B GPT3.5 GPT4o\nFundamental ScienceAverage 32.3 48.3 58.5 70.3 74.8 70.9 77.1 78.4 62.2 76.7\nMMLU 35.5 52.1 59.4 64.0 65.3 76.2 78.7 78.5 64.3 84.2\nCMMLU 27.5 31.8 46.1 79.2 87.8 65.5 86.6 89.6 44.9 78.7\nXz-Ch 34.6 51.7 64.7 71.7 75.4 73.4 74.0 75.5 73.2 73.4\nXz-En 31.5 57.6 63.6 66.2 70.7 68.4 69.1 69.9 66.5 70.3\nAlloy MaterialsAverage 23.9 28.0 32.9 32.8 35.6 44.9 42.7 49.1 32.0 52.1\nAlloyQA 6.7 33.3 26.7 53.3 53.3 46.7 53.3 66.7 53.3 46.7\nCompEx 9.0 9.0 8.1 10.1 7.2 34.7 19.3 19.5 24.8 50.5\nTempEx 34.3 28.5 32.9 28.5 30.9 55.6 58.0 57.9 30.9 60.9\nSampDiff 27.4 14.3 29.1 6.3 18.1 26.6 7.6 32.8 12.7 34.6\nTreatSeq 42.2 54.9 67.6 65.7 68.6 60.8 75.5 68.6 38.2 67.6\nBiomedicineAverage 67.8 76.0 77.4 80.8 79.6 79.6 81.0 79.6 78.0 82.3\nBioQA 33.3 37.4 43.4 41.4 38.4 50.5 55.6 54.5 29.3 59.6\nChemER 68.3 93.2 84.3 92.1 90.5 86.1 90.9 91.4 93.3 90.3\nDisER 80.8 82.2 80.9 87.2 88.4 79.8 81.7 80.9 90.5 81.1\nCompDis 67.7 70.3 74.6 73.8 74.5 78.2 76.3 74.0 71.6 72.6\nGeneFunc 70.9 79.9 88.8 92.9 89.6 87.1 85.5 81.3 88.8 96.4\nGeneReg 85.9 92.8 92.1 97.1 95.9 95.9 95.9 95.7 94.2 93.7\nDrug DiscoveryAverage 25.4 30.2 32.0 31.7 33.2 41.5 35.5 41.8 31.0 43.4\nAffEx 1.1 4.6 5.7 4.3 3.0 3.5 6.1 5.4 8.1 31.4\nDrugQA 40.0 53.3 33.3 20.0 46.7 40.0 33.3 40.0 33.3 53.3\nTagMol 7.3 0.0 10.2 1.5 20.1 23.0 13.0 27.1 0.6 7.8\nMarkMol 28.4 15.9 18.0 34.4 32.8 53.3 31.7 52.4 48.8 63.8\nMolDoc 44.0 46.0 50.0 56.0 56.0 48.0 50.0 58.0 44.0 54.0\nReactQA 25.3 25.3 32.6 28.4 26.3 50.5 36.8 32.6 34.7 37.9\nResTarg 31.9 66.2 74.3 77.0 47.7 72.3 77.3 77.3 47.4 55.7\nOrganic MaterialsAverage 16.7 20.6 24.5 28.3 38.9 41.5 52.7 48.6 24.4 62.7\nElecQA 26.0 20.0 41.0 30.0 28.0 33.0 49.0 33.0 26.0 68.0\nOLEDEx 1.8 8.0 6.5 7.2 6.8 16.1 17.4 11.0 13.5 27.9\nPolyQA 6.7 6.7 13.3 20.0 93.3 80.0 73.3 80.0 0.0 80.0\nPolyCompQA 23.9 25.7 35.8 32.1 49.5 53.2 73.4 74.9 33.0 82.6\nPolyPropEx 4.9 20.2 22.4 32.2 43.4 35.9 54.4 54.4 39.5 75.3\nSolEx 26.2 31.8 34.0 35.7 32.9 36.2 42.3 38.5 35.8 45.9\nReactMechQA 27.3 31.8 18.2 40.9 18.2 36.4 59.1 48.2 22.7 59.1\nOverall Average 33.2 40.6 45.0 48.8 52.4 55.7 57.8 59.5 45.5 63.4\nTable 7: Detailed model performance on SciAssess tasks. SciLitLLM-7B shows significant improvement in the Fundamental\nScience and Organic Materials domains while maintaining comparable performance in other domains. Overall, SciLitLLM-7B\nachieves approximately 3.6% improvement over the second-best LLM.", "meta_data": {"source": "SciLitLLM.pdf"}}
